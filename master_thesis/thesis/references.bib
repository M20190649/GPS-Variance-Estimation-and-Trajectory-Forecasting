@article{Anselin1999,
  title={Interactive techniques and exploratory spatial data analysis},
  author={Anselin, Luc},
  journal={Geographical Information Systems: principles, techniques, management and applications},
  volume={1},
  pages={251--264},
  year={1999}
}

@ARTICLE{Baptista2018,
  title = {Forecasting fault events for predictive maintenance using data-driven techniques and ARMA modeling},
  journal = {Computers \& Industrial Engineering},
  volume = {115},
  pages = {41 - 53},
  year = {2018},
  issn = {0360-8352},
  doi = {https://doi.org/10.1016/j.cie.2017.10.033},
  url = {http://www.sciencedirect.com/science/article/pii/S036083521730520X},
  author = {Marcia Baptista and Shankar Sankararaman and Ivo. P. de Medeiros and Cairo Nascimento and Helmut Prendinger and Elsa M.P. Henriques},
  keywords = {Real case study, Aircraft prognostics, Predictive maintenance, Data-driven techniques, ARMA modeling, Life usage modeling}
}

@book{Bishop2006,
  author = {Bishop, Christopher M.},
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  year = {2006},
  isbn = {0387310738},
  publisher = {Springer-Verlag New York, Inc.},
  address = {Secaucus, NJ, USA},
}

@article{Bradley1997,
  title = "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
  journal = "Pattern Recognition",
  volume = "30",
  number = "7",
  pages = "1145 - 1159",
  year = "1997",
  issn = "0031-3203",
  doi = "https://doi.org/10.1016/S0031-3203(96)00142-2",
  url = "http://www.sciencedirect.com/science/article/pii/S0031320396001422",
  author = "Andrew P. Bradley",
  keywords = "The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error",
  abstract = "Abstract In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six Ã¢ÂÂreal worldÃ¢ÂÂ medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for Ã¢ÂÂsingle numberÃ¢ÂÂ evaluation of machine learning algorithms."
}


@Article{Breiman2001,
  author="Breiman, Leo",
  title="Random Forests",
  journal="Machine Learning",
  year="2001",
  month="Oct",
  day="01",
  volume="45",
  number="1",
  pages="5--32",
  abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
  issn="1573-0565",
  doi="10.1023/A:1010933404324",
  url="https://doi.org/10.1023/A:1010933404324"
}


@Article{Cortes1995,
  author="Cortes, Corinna
  and Vapnik, Vladimir",
  title="Support-Vector Networks",
  journal="Machine Learning",
  year="1995",
  month="Sep",
  day="01",
  volume="20",
  number="3",
  pages="273--297",
  abstract="The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
  issn="1573-0565",
  doi="10.1023/A:1022627411411",
  url="https://doi.org/10.1023/A:1022627411411"
}

@article {Gelman2003,
  author = {Gelman, Andrew},
  title = {A Bayesian Formulation of Exploratory Data Analysis and Goodness-of-fit Testing*},
  journal = {International Statistical Review},
  volume = {71},
  number = {2},
  publisher = {Blackwell Publishing Ltd},
  issn = {1751-5823},
  url = {http://dx.doi.org/10.1111/j.1751-5823.2003.tb00203.x},
  doi = {10.1111/j.1751-5823.2003.tb00203.x},
  pages = {369--382},
  keywords = {Bootstrap, Fisher's exact test, Graphics, Mixture model, Model checking, Multiple imputation, Prior predictive check, Posterior predictive check, p-value, u-value, Bootstrap, Fisher's exact test, Graphiques, ModÃ©les de mÃšlange, VÃ©rification de modÃ©le, VÃ©rification prÃ©dictive antÃ©rieur, VÃ©rification prÃ©dictive a posteriori, âp-valueâ, âu-valueâ},
  year = {2003},
}

@INPROCEEDINGS{Ho1995, 
  author={Tin Kam Ho}, 
  booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
  title={Random decision forests}, 
  year={1995}, 
  volume={1}, 
  number={}, 
  pages={278-282 vol.1}, 
  abstract={Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits}, 
  keywords={decision theory;handwriting recognition;optical character recognition;complexity;decision trees;generalization accuracy;handwritten digits;random decision forests;stochastic modeling;suboptimal accuracy;tree-based classifiers;Classification tree analysis;Decision trees;Handwriting recognition;Hidden Markov models;Multilayer perceptrons;Optimization methods;Stochastic processes;Testing;Tin;Training data}, 
  doi={10.1109/ICDAR.1995.598994}, 
  ISSN={}, 
  month={Aug},
}

@ARTICLE{Ho1998, 
  author={Tin Kam Ho}, 
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The random subspace method for constructing decision forests}, 
  year={1998}, 
  volume={20}, 
  number={8}, 
  pages={832-844}, 
  abstract={Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy}, 
  keywords={decision theory;learning (artificial intelligence);pattern classification;random processes;trees (mathematics);classification accuracy;decision forests;decision tree based classifier;decision trees;feature vector;generalization accuracy;maximum accuracy;overfitting;random subspace method;Binary trees;Classification tree analysis;Clustering algorithms;Decision trees;Stochastic systems;Support vector machine classification;Support vector machines;Tin;Training data}, 
  doi={10.1109/34.709601}, 
  ISSN={0162-8828}, 
  month={Aug},
}

@article{Hoaglin2003,
  ISSN = {08834237},
  URL = {http://www.jstor.org/stable/3182748},
  abstract = {From the time that John W. Tukey started to do serious work in statistics, he was interested in problems and techniques of data analysis. Some people know him best for exploratory data analysis, which he pioneered, but he also made key contributions in analysis of variance, in regression and through a wide range of applications. This paper reviews illustrative contributions in these areas.},
  author = {David C. Hoaglin},
  journal = {Statistical Science},
  number = {3},
  pages = {311-318},
  publisher = {Institute of Mathematical Statistics},
  title = {John W. Tukey and Data Analysis},
  volume = {18},
  year = {2003}
}

@incollection{Jolliffe2002,
  title={Principal Component Analysis},
  author={Jolliffe, Ian T},
  booktitle={Principal Component Analysis, Second Edition},
  ISBN={0-387-95442-2},
  year={2002},
  publisher={Springer}
}

@incollection{Krizhevsky2012,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages = {1097--1105},
  year = {2012},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@INPROCEEDINGS{Langone2013, 
  author={R. Langone and C. Alzate and B. De Ketelaere and J. A. K. Suykens}, 
  booktitle={2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}, 
  title={Kernel spectral clustering for predicting maintenance of industrial machines}, 
  year={2013}, 
  volume={}, 
  number={}, 
  pages={39-45}, 
  keywords={fault diagnosis;learning (artificial intelligence);maintenance engineering;mechanical engineering computing;pattern clustering;production equipment;safety;KSC;fault detection;industrial machines;kernel spectral clustering;maintenance prediction;manufacturing cost reduction;out-of-sample ability;plant operation safety;sensor data;state-of-the-art unsupervised learning technique;systematic model selection scheme;Accelerometers;Indexes;Kernel;Maintenance engineering;Monitoring;Principal component analysis;Tuning}, 
  doi={10.1109/CIDM.2013.6597215}, 
  ISSN={}, 
  month={April},
}

@inproceedings{LeCun1995,
original =    "orig/lecun-bengio-95a.ps.gz",
author  =       "LeCun, Y. and Bengio, Y.",
title   =       "Convolutional Networks for Images, Speech, and Time-Series",
booktitle=      "The Handbook of Brain Theory and Neural Networks",
year    =       "1995",
editor  =       "Arbib, M. A.",
publisher=      "MIT Press",
}

@inproceedings{Pang2002,
  author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
  booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
  series = {EMNLP '02},
  year = {2002},
  pages = {79--86},
  numpages = {8},
  url = {https://doi.org/10.3115/1118693.1118704},
  doi = {10.3115/1118693.1118704},
  acmid = {1118704},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
} 


@inproceedings{Quinlan1992,
  title={Learning with continuous classes},
  author={Quinlan, John R and others},
  booktitle={5th Australian joint conference on artificial intelligence},
  volume={92},
  pages={343--348},
  year={1992},
  organization={Singapore}
}


@ARTICLE{Susto2015, 
  author={G. A. Susto and A. Schirru and S. Pampuri and S. McLoone and A. Beghi}, 
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Machine Learning for Predictive Maintenance: A Multiple Classifier Approach}, 
  year={2015}, 
  volume={11}, 
  number={3}, 
  pages={812-820}, 
  abstract={In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.}, 
  keywords={data mining;learning (artificial intelligence);pattern classification;production engineering computing;semiconductor device manufacture;PdM;censored data problem;data mining;dynamical decision rules;health factors;high-dimensional problem;maintenance management;multiple classifier machine learning methodology;operating cost-based maintenance decision system;predictive maintenance;quantitative indicators;semiconductor manufacturing maintenance problem;Availability;Informatics;Manufacturing;Predictive maintenance;Production;Training;Classification algorithms;data mining;ion implantation;machine learning (ML);predictive maintenance (PdM);semiconductor device manufacture}, 
  doi={10.1109/TII.2014.2349359}, 
  ISSN={1551-3203}, 
  month={June},
}

@article{Tukey1977,
  title={Exploratory data analysis},
  author={Tukey, John W},
  year={1977},
  publisher={Reading, Mass.}
}


@book{Velleman1981,
  title={Applications, basics, and computing of exploratory data analysis},
  author={Velleman, Paul F and Hoaglin, David C},
  year={1981},
  publisher={Duxbury Press},
  isbn={0-87150-409-X},
  url={http://hdl.handle.net/1813/78},
  abstract={This book provides an introduction to the methods
  of exploratory data analysis as originally developed by John Tukey. For each of nine methods it discusses and illustrates the methods, provides background derivations and explanations, and presents programs in Fortran and BASIC. For most of these methods, this was the first source of such programs.},
}



