@online{current-android-malware,
  author = {Spreitzenbarth},
  title = {Current Android Malware},
  year = 2016,
  url = {https://forensics.spreitzenbarth.de/android-malware/},
  urldate = {2017-11-13}
}

@online{android-security-2015,
  author = {Google},
  title = {{Android Security} 2015 Year In Review},
  year = 2016,
  url = {https://source.android.com/security/reports/Google_Android_Security_2015_Report_Final.pdf},
  urldate = {2017-11-13}
}

@incollection{Krizhevsky2012,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@inproceedings{LeCun95a,
original =    "orig/lecun-bengio-95a.ps.gz",
author  =       "LeCun, Y. and Bengio, Y.",
title   =       "Convolutional Networks for Images, Speech, and Time-Series",
booktitle=      "The Handbook of Brain Theory and Neural Networks",
year    =       "1995",
editor  =       "Arbib, M. A.",
publisher=      "MIT Press",
}


@online{android-security-2016,
  author = {Google},
  title = {{Android Security} 2016 Year In Review},
  year = 2017,
  url = {https://source.android.com/security/reports/Google_Android_Security_2016_Report_Final.pdf},
  urldate = {2017-11-13}
}

@Article{Shang2017,
author="Shang, Fengjun
and Li, Yalin
and Deng, Xiaolin
and He, Dexiang",
title="Android malware detection method based on naive Bayes and permission correlation algorithm",
journal="Cluster Computing",
year="2017",
month="Jun",
day="17",
abstract="In order to detect Android malware more effectively, an Android malware detection model was proposed based on improved naive Bayes classification. Firstly, considering the unknown permission that may be malicious in detection samples, and in order to improve the Android detection rate, the algorithm of malware detection is proposed based on improved naive Bayes. Considering the limited training samples, limited permissions, and the new malicious permissions in the test samples, we used the impact of the new malware permissions and training permissions as the weight. The weighted naive Bayesian algorithm improves the Android malware detection efficiency. Secondly, taking into account the detection model, we proposed a detection model of permissions and information theory based on the improved naive Bayes algorithm. We analyzed the correlation of the permission. By calculating the Pearson correlation coefficient, we determined the value of Pearson correlation coefficient r, and delete the permissions whose value r is less than the threshold                                                                   {\$}{\$}{\backslash}rho {\$}{\$}                                                      $\rho$                                                 and get the new permission set. So, we got the improved detection model by clustering based on information theory. Finally, we detected the 1725 Android malware and 945 non malicious application of multiple data sets in the same simulation environment. The detection rate of the improved the naive Bayes algorithm is 86.54{\%}, and the detection rate of the non-malicious application is increased to 97.59{\%}. Based on the improved naive Bayes algorithm, the false detection rate of the improved detection model is reduced by 8.25{\%}.",
issn="1573-7543",
doi="10.1007/s10586-017-0981-6",
url="https://doi.org/10.1007/s10586-017-0981-6"
}

@inproceedings{McLaughlin2017,
 author = {McLaughlin, Niall and Martinez del Rincon, Jesus and Kang, BooJoong and Yerima, Suleiman and Miller, Paul and Sezer, Sakir and Safaei, Yeganeh and Trickel, Erik and Zhao, Ziming and Doup{\'e}, Adam and Joon Ahn, Gail},
 title = {Deep Android Malware Detection},
 booktitle = {Proceedings of the Seventh ACM on Conference on Data and Application Security and Privacy},
 series = {CODASPY '17},
 year = {2017},
 isbn = {978-1-4503-4523-1},
 location = {Scottsdale, Arizona, USA},
 pages = {301--308},
 numpages = {8},
 url = {http://doi.acm.org.e.bibl.liu.se/10.1145/3029806.3029823},
 doi = {10.1145/3029806.3029823},
 acmid = {3029823},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {android, deep learning, malware detection},
}

@online{metamorphic-virology,
  author = {Gregory Disney-Leugers},
  title = {Metamorphic Virolog},
  year = 2013,
  url = {https://www.owasp.org/index.php/Metamorphic_Virology},
  urldate = {2017-11-13}
}

@online{PHA,
  author = {Google},
  title = {The Google Android Security Team's Classifications for Potentially Harmful Applications},
  year = 2017,
  url = {https://source.android.com/security/reports/Google_Android_Security_PHA_classifications.pdf},
  urldate = {2017-11-14}
}

@article{Tukey1977,
  title={Exploratory data analysis},
  author={Tukey, John W},
  year={1977},
  publisher={Reading, Mass.}
}

@article {Gelman2003,
author = {Gelman, Andrew},
title = {A Bayesian Formulation of Exploratory Data Analysis and Goodness-of-fit Testing*},
journal = {International Statistical Review},
volume = {71},
number = {2},
publisher = {Blackwell Publishing Ltd},
issn = {1751-5823},
url = {http://dx.doi.org/10.1111/j.1751-5823.2003.tb00203.x},
doi = {10.1111/j.1751-5823.2003.tb00203.x},
pages = {369--382},
keywords = {Bootstrap, Fisher's exact test, Graphics, Mixture model, Model checking, Multiple imputation, Prior predictive check, Posterior predictive check, p-value, u-value, Bootstrap, Fisher's exact test, Graphiques, Modéles de mèlange, Vérification de modéle, Vérification prédictive antérieur, Vérification prédictive a posteriori, “p-value”, “u-value”},
year = {2003},
}

@article{Anselin1999,
  title={Interactive techniques and exploratory spatial data analysis},
  author={Anselin, Luc},
  journal={Geographical Information Systems: principles, techniques, management and applications},
  volume={1},
  pages={251--264},
  year={1999}
}

@online{Nokia2016,
title={Are our smartphones the new attack vector for hackers?},
author={Cassie Phillips},
year=2016,
url={https://blog.networks.nokia.com/mobile-networks/2016/07/04/smartphones-new-attack-vector-hackers/},
urldate={2017-12-19},
}

@ARTICLE{Shore1980, 
author={J. Shore and R. Johnson}, 
journal={IEEE Transactions on Information Theory}, 
title={Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}, 
year={1980}, 
volume={26}, 
number={1}, 
pages={26-37}, 
abstract={Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.}, 
keywords={Entropy functions;Computer network reliability;Computer networks;Entropy;Modeling;Physics;Queueing analysis;Spectral analysis;Statistics;Telecommunication traffic;Traffic control}, 
doi={10.1109/TIT.1980.1056144}, 
ISSN={0018-9448}, 
month={Jan},}

@inproceedings{Kohavi1995,
  title={A study of cross-validation and bootstrap for accuracy estimation and model selection},
  author={Kohavi, Ron and others},
  booktitle={Ijcai},
  volume={14},
  number={2},
  pages={1137--1145},
  year={1995},
  organization={Stanford, CA}
}

@inproceedings{Caruana2006,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {An Empirical Comparison of Supervised Learning Algorithms},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 series = {ICML '06},
 year = {2006},
 isbn = {1-59593-383-2},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {161--168},
 numpages = {8},
 url = {http://doi.acm.org.e.bibl.liu.se/10.1145/1143844.1143865},
 doi = {10.1145/1143844.1143865},
 acmid = {1143865},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Bradley1997,
title = "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
journal = "Pattern Recognition",
volume = "30",
number = "7",
pages = "1145 - 1159",
year = "1997",
issn = "0031-3203",
doi = "https://doi.org/10.1016/S0031-3203(96)00142-2",
url = "http://www.sciencedirect.com/science/article/pii/S0031320396001422",
author = "Andrew P. Bradley",
keywords = "The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error",
abstract = "Abstract In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six âreal worldâ medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for âsingle numberâ evaluation of machine learning algorithms."
}

@inproceedings{Pang2002,
 author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
 title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
 booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 series = {EMNLP '02},
 year = {2002},
 pages = {79--86},
 numpages = {8},
 url = {https://doi.org/10.3115/1118693.1118704},
 doi = {10.3115/1118693.1118704},
 acmid = {1118704},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@inproceedings{Quinlan1992,
  title={Learning with continuous classes},
  author={Quinlan, John R and others},
  booktitle={5th Australian joint conference on artificial intelligence},
  volume={92},
  pages={343--348},
  year={1992},
  organization={Singapore}
}

@article{Hoaglin2003,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/3182748},
 abstract = {From the time that John W. Tukey started to do serious work in statistics, he was interested in problems and techniques of data analysis. Some people know him best for exploratory data analysis, which he pioneered, but he also made key contributions in analysis of variance, in regression and through a wide range of applications. This paper reviews illustrative contributions in these areas.},
 author = {David C. Hoaglin},
 journal = {Statistical Science},
 number = {3},
 pages = {311-318},
 publisher = {Institute of Mathematical Statistics},
 title = {John W. Tukey and Data Analysis},
 volume = {18},
 year = {2003}
}

@book{Velleman1981,
  title={Applications, basics, and computing of exploratory data analysis},
  author={Velleman, Paul F and Hoaglin, David C},
  year={1981},
  publisher={Duxbury Press},
  isbn={0-87150-409-X},
  url={http://hdl.handle.net/1813/78},
  abstract={This book provides an introduction to the methods
 of exploratory data analysis as originally developed by John Tukey. For each of nine methods it discusses and illustrates the methods, provides background derivations and explanations, and presents programs in Fortran and BASIC. For most of these methods, this was the first source of such programs.},
}

@Inbook{Lewis1998,
author="Lewis, David D.",
editor="N{\'e}dellec, Claire
and Rouveirol, C{\'e}line",
title="Naive (Bayes) at forty: The independence assumption in information retrieval",
bookTitle="Machine Learning: ECML-98: 10th European Conference on Machine Learning Chemnitz, Germany, April 21--23, 1998 Proceedings",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="4--15",
abstract="The naive Bayes classifier, currently experiencing a renaissance ] in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.",
isbn="978-3-540-69781-7",
doi="10.1007/BFb0026666",
url="https://doi.org/10.1007/BFb0026666"
}

@ARTICLE{Yerima2015, 
author={S. Y. Yerima and S. Sezer and I. Muttik}, 
journal={IET Information Security}, 
title={High accuracy android malware detection using ensemble learning}, 
year={2015}, 
volume={9}, 
number={6}, 
pages={313-320}, 
keywords={Android (operating system);invasive software;learning (artificial intelligence);ensemble machine learning;high accuracy Android malware detection;static analysis}, 
doi={10.1049/iet-ifs.2014.0099}, 
ISSN={1751-8709}, 
month={},
}

@INPROCEEDINGS{Liang2014, 
author={S. Liang and X. Du}, 
booktitle={2014 IEEE International Conference on Communications (ICC)}, 
title={Permission-combination-based scheme for Android mobile malware detection}, 
year={2014}, 
volume={}, 
number={}, 
pages={2301-2306}, 
keywords={invasive software;smart phones;Android mobile malware detection;Android mobile phones;permission-combination-based scheme;Androids;Humanoid robots;Internet;Malware;Mobile communication;Mobile computing;Smart phones;Android;malware detection;mobile phone}, 
doi={10.1109/ICC.2014.6883666}, 
ISSN={1550-3607}, 
month={June},
}

@article{Tong2002,
 author = {Tong, Simon and Koller, Daphne},
 title = {Support Vector Machine Active Learning with Applications to Text Classification},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2002},
 volume = {2},
 month = mar,
 year = {2002},
 issn = {1532-4435},
 pages = {45--66},
 numpages = {22},
 url = {http://dx.doi.org/10.1162/153244302760185243},
 doi = {10.1162/153244302760185243},
 acmid = {944793},
 publisher = {JMLR.org},
 keywords = {active learning, classification, relevance feedback, selective sampling, support vector machines},
} 


@Article{Cortes1995,
author="Cortes, Corinna
and Vapnik, Vladimir",
title="Support-Vector Networks",
journal="Machine Learning",
year="1995",
month="Sep",
day="01",
volume="20",
number="3",
pages="273--297",
abstract="The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
issn="1573-0565",
doi="10.1023/A:1022627411411",
url="https://doi.org/10.1023/A:1022627411411"
}

@Inbook{Zhao2011,
author="Zhao, Min
and Ge, Fangbin
and Zhang, Tao
and Yuan, Zhijian",
editor="Liu, Chunfeng
and Chang, Jincai
and Yang, Aimin",
title="AntiMalDroid: An Efficient SVM-Based Malware Detection Framework for Android",
bookTitle="Information Computing and Applications: Second International Conference, ICICA 2011, Qinhuangdao, China, October 28-31, 2011. Proceedings, Part I",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="158--166",
abstract="Mobile handsets, especially smartphones, are becoming more open and general-purpose, thus they also become attack targets of malware. Threat of malicious software has become an important factor in the safety of smartphones. Android is the most popular open-source smartphone operating system and its permission declaration access control mechanisms can't detect the behavior of malware. In this work, AntiMalDroid, a software behavior signature based malware detection framework using SVM algorithm is proposed, AntiMalDroid can detect malicious software and there variants effectively in runtime and extend malware characteristics database dynamically. Experimental results show that the approach has high detection rate and low rate of false positive and false negative, the power and performance impact on the original system can also be ignored.",
isbn="978-3-642-27503-6",
doi="10.1007/978-3-642-27503-6_22",
url="https://doi.org/10.1007/978-3-642-27503-6_22"
}

@book{Bishop2006,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 

@INPROCEEDINGS{Alam2013, 
author={M. S. Alam and S. T. Vuong}, 
booktitle={2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing}, 
title={Random Forest Classification for Detecting Android Malware}, 
year={2013}, 
volume={}, 
number={}, 
pages={663-669}, 
abstract={Internet connected smartphone devices play a crucial role in the application domain of Internet of Things. These devices are being widely used for day-to-day activities such as remotely controlling lighting and heating at homes, paying for parking, and recently for paying for goods using saved credit card information using Near Field Communication (NFC). Android is the most popular smartphone platform today. It is also the choice of malware authors to obtain secure and private data. In this paper we exclusively apply the machine learning ensemble learning algorithm Random Forest supervised classifier on an Android feature dataset of 48919 points of 42 features each. Our goal was to measure the accuracy of Random Forest in classifying Android application behavior to classify applications as malicious or benign. Moreover, we wanted to focus on detection accuracy as the free parameters of the Random Forest algorithm such as the number of trees, depth of each tree and number of random features selected are varied. Our experimental results based on 5-fold cross validation of our dataset shows that Random Forest performs very well with an accuracy of over 99 percent in general, an optimal Out-Of-Bag (OOB) error rate [3] of 0.0002 for forests with 40 trees or more, and a root mean squared error of 0.0171 for 160 trees.}, 
keywords={Android (operating system);Internet of Things;data privacy;feature selection;invasive software;learning (artificial intelligence);pattern classification;smart phones;Android application behavior;Android feature dataset;Android malware detection;Internet connected smartphone devices;Internet of Things;application domain;machine learning ensemble learning algorithm;optimal out-of-bag error rate;random feature selection;random forest algorithm;random forest classification;random forest supervised classifier;root mean squared error;Androids;Error analysis;Humanoid robots;Malware;Smart phones;Vectors;Vegetation;Android;Machine Learning;Malware}, 
doi={10.1109/GreenCom-iThings-CPSCom.2013.122}, 
ISSN={}, 
month={Aug},
}

@INPROCEEDINGS{Ho1995, 
author={Tin Kam Ho}, 
booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
title={Random decision forests}, 
year={1995}, 
volume={1}, 
number={}, 
pages={278-282 vol.1}, 
abstract={Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits}, 
keywords={decision theory;handwriting recognition;optical character recognition;complexity;decision trees;generalization accuracy;handwritten digits;random decision forests;stochastic modeling;suboptimal accuracy;tree-based classifiers;Classification tree analysis;Decision trees;Handwriting recognition;Hidden Markov models;Multilayer perceptrons;Optimization methods;Stochastic processes;Testing;Tin;Training data}, 
doi={10.1109/ICDAR.1995.598994}, 
ISSN={}, 
month={Aug},
}

@Article{Breiman2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}

@ARTICLE{Ho1998, 
author={Tin Kam Ho}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={The random subspace method for constructing decision forests}, 
year={1998}, 
volume={20}, 
number={8}, 
pages={832-844}, 
abstract={Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy}, 
keywords={decision theory;learning (artificial intelligence);pattern classification;random processes;trees (mathematics);classification accuracy;decision forests;decision tree based classifier;decision trees;feature vector;generalization accuracy;maximum accuracy;overfitting;random subspace method;Binary trees;Classification tree analysis;Clustering algorithms;Decision trees;Stochastic systems;Support vector machine classification;Support vector machines;Tin;Training data}, 
doi={10.1109/34.709601}, 
ISSN={0162-8828}, 
month={Aug},}

@inproceedings{Mahindru2017,
 author = {Mahindru, Arvind and Singh, Paramvir},
 title = {Dynamic Permissions Based Android Malware Detection Using Machine Learning Techniques},
 booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
 series = {ISEC '17},
 year = {2017},
 isbn = {978-1-4503-4856-0},
 location = {Jaipur, India},
 pages = {202--210},
 numpages = {9},
 url = {http://doi.acm.org.e.bibl.liu.se/10.1145/3021460.3021485},
 doi = {10.1145/3021460.3021485},
 acmid = {3021485},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Android, Dynamic Analysis, Machine Learning, Malware Detection},
} 
