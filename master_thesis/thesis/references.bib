@article{Anselin1999,
  title={Interactive techniques and exploratory spatial data analysis},
  author={Anselin, Luc},
  journal={Geographical Information Systems: principles, techniques, management and applications},
  volume={1},
  pages={251--264},
  year={1999}
}

@ARTICLE{Baptista2018,
  title = {Forecasting fault events for predictive maintenance using data-driven techniques and ARMA modeling},
  journal = {Computers \& Industrial Engineering},
  volume = {115},
  pages = {41 - 53},
  year = {2018},
  issn = {0360-8352},
  doi = {https://doi.org/10.1016/j.cie.2017.10.033},
  url = {http://www.sciencedirect.com/science/article/pii/S036083521730520X},
  author = {Marcia Baptista and Shankar Sankararaman and Ivo. P. de Medeiros and Cairo Nascimento and Helmut Prendinger and Elsa M.P. Henriques},
  keywords = {Real case study, Aircraft prognostics, Predictive maintenance, Data-driven techniques, ARMA modeling, Life usage modeling}
}

@book{Bishop2006,
  author = {Bishop, Christopher M.},
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  year = {2006},
  isbn = {0387310738},
  publisher = {Springer-Verlag New York, Inc.},
  address = {Secaucus, NJ, USA},
}

@article{Bradley1997,
  title = "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
  journal = "Pattern Recognition",
  volume = "30",
  number = "7",
  pages = "1145 - 1159",
  year = "1997",
  issn = "0031-3203",
  doi = "https://doi.org/10.1016/S0031-3203(96)00142-2",
  url = "http://www.sciencedirect.com/science/article/pii/S0031320396001422",
  author = "Andrew P. Bradley",
  keywords = "The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error",
  abstract = "Abstract In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six Ã¢ÂÂreal worldÃ¢ÂÂ medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for Ã¢ÂÂsingle numberÃ¢ÂÂ evaluation of machine learning algorithms."
}


@Article{Breiman2001,
  author="Breiman, Leo",
  title="Random Forests",
  journal="Machine Learning",
  year="2001",
  month="Oct",
  day="01",
  volume="45",
  number="1",
  pages="5--32",
  abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
  issn="1573-0565",
  doi="10.1023/A:1010933404324",
  url="https://doi.org/10.1023/A:1010933404324"
}


@Article{Cortes1995,
  author="Cortes, Corinna
  and Vapnik, Vladimir",
  title="Support-Vector Networks",
  journal="Machine Learning",
  year="1995",
  month="Sep",
  day="01",
  volume="20",
  number="3",
  pages="273--297",
  abstract="The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
  issn="1573-0565",
  doi="10.1023/A:1022627411411",
  url="https://doi.org/10.1023/A:1022627411411"
}

@article {Gelman2003,
  author = {Gelman, Andrew},
  title = {A Bayesian Formulation of Exploratory Data Analysis and Goodness-of-fit Testing*},
  journal = {International Statistical Review},
  volume = {71},
  number = {2},
  publisher = {Blackwell Publishing Ltd},
  issn = {1751-5823},
  url = {http://dx.doi.org/10.1111/j.1751-5823.2003.tb00203.x},
  doi = {10.1111/j.1751-5823.2003.tb00203.x},
  pages = {369--382},
  keywords = {Bootstrap, Fisher's exact test, Graphics, Mixture model, Model checking, Multiple imputation, Prior predictive check, Posterior predictive check, p-value, u-value, Bootstrap, Fisher's exact test, Graphiques, ModÃ©les de mÃšlange, VÃ©rification de modÃ©le, VÃ©rification prÃ©dictive antÃ©rieur, VÃ©rification prÃ©dictive a posteriori, âp-valueâ, âu-valueâ},
  year = {2003},
}

@INPROCEEDINGS{Ho1995, 
  author={Tin Kam Ho}, 
  booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
  title={Random decision forests}, 
  year={1995}, 
  volume={1}, 
  number={}, 
  pages={278-282 vol.1}, 
  abstract={Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits}, 
  keywords={decision theory;handwriting recognition;optical character recognition;complexity;decision trees;generalization accuracy;handwritten digits;random decision forests;stochastic modeling;suboptimal accuracy;tree-based classifiers;Classification tree analysis;Decision trees;Handwriting recognition;Hidden Markov models;Multilayer perceptrons;Optimization methods;Stochastic processes;Testing;Tin;Training data}, 
  doi={10.1109/ICDAR.1995.598994}, 
  ISSN={}, 
  month={Aug},
}

@ARTICLE{Ho1998, 
  author={Tin Kam Ho}, 
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={The random subspace method for constructing decision forests}, 
  year={1998}, 
  volume={20}, 
  number={8}, 
  pages={832-844}, 
  abstract={Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy}, 
  keywords={decision theory;learning (artificial intelligence);pattern classification;random processes;trees (mathematics);classification accuracy;decision forests;decision tree based classifier;decision trees;feature vector;generalization accuracy;maximum accuracy;overfitting;random subspace method;Binary trees;Classification tree analysis;Clustering algorithms;Decision trees;Stochastic systems;Support vector machine classification;Support vector machines;Tin;Training data}, 
  doi={10.1109/34.709601}, 
  ISSN={0162-8828}, 
  month={Aug},
}

@article{Hoaglin2003,
  ISSN = {08834237},
  URL = {http://www.jstor.org/stable/3182748},
  abstract = {From the time that John W. Tukey started to do serious work in statistics, he was interested in problems and techniques of data analysis. Some people know him best for exploratory data analysis, which he pioneered, but he also made key contributions in analysis of variance, in regression and through a wide range of applications. This paper reviews illustrative contributions in these areas.},
  author = {David C. Hoaglin},
  journal = {Statistical Science},
  number = {3},
  pages = {311-318},
  publisher = {Institute of Mathematical Statistics},
  title = {John W. Tukey and Data Analysis},
  volume = {18},
  year = {2003}
}

@incollection{Jolliffe2002,
  title={Principal Component Analysis},
  author={Jolliffe, Ian T},
  booktitle={Principal Component Analysis, Second Edition},
  ISBN={0-387-95442-2},
  year={2002},
  publisher={Springer}
}

@Article{Knuth1971,
author="Knuth, D. E.",
title="Optimum binary search trees",
journal="Acta Informatica",
year="1971",
month="03",
day="01",
volume="1",
number="1",
pages="14--25",
issn="1432-0525",
doi="10.1007/BF00264289",
url="https://doi.org/10.1007/BF00264289"
}

@book{Kozen1997,
 author = {Kozen, Dexter C.},
 title = {Automata and Computability},
 year = {1997},
 isbn = {0-387-94907-0},
 edition = {1st},
 publisher = {Springer Science+Business Media, Inc.},
 address = {233 Spring Street, New York, NY 10013, USA},
} 

@incollection{Krizhevsky2012,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages = {1097--1105},
  year = {2012},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@INPROCEEDINGS{Langone2013, 
  author={R. Langone and C. Alzate and B. De Ketelaere and J. A. K. Suykens}, 
  booktitle={2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}, 
  title={Kernel spectral clustering for predicting maintenance of industrial machines}, 
  year={2013}, 
  volume={}, 
  number={}, 
  pages={39-45}, 
  keywords={fault diagnosis;learning (artificial intelligence);maintenance engineering;mechanical engineering computing;pattern clustering;production equipment;safety;KSC;fault detection;industrial machines;kernel spectral clustering;maintenance prediction;manufacturing cost reduction;out-of-sample ability;plant operation safety;sensor data;state-of-the-art unsupervised learning technique;systematic model selection scheme;Accelerometers;Indexes;Kernel;Maintenance engineering;Monitoring;Principal component analysis;Tuning}, 
  doi={10.1109/CIDM.2013.6597215}, 
  ISSN={}, 
  month={April},
}

@inproceedings{LeCun1995,
original =    "orig/lecun-bengio-95a.ps.gz",
author  =       "LeCun, Y. and Bengio, Y.",
title   =       "Convolutional Networks for Images, Speech, and Time-Series",
booktitle=      "The Handbook of Brain Theory and Neural Networks",
year    =       "1995",
editor  =       "Arbib, M. A.",
publisher=      "MIT Press",
}

@inproceedings{Pang2002,
  author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
  booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
  series = {EMNLP '02},
  year = {2002},
  pages = {79--86},
  numpages = {8},
  url = {https://doi.org/10.3115/1118693.1118704},
  doi = {10.3115/1118693.1118704},
  acmid = {1118704},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
} 
@ARTICLE{GPflow2017,
author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{‘o}n-Villagr{‘a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
title = {GPflow: A Gaussian process library using TensorFlow},
journal = {Journal of Machine Learning Research},
year = {2017},
month = {apr},
volume = {18},
number = {40},
pages = {1-6},
url = {http://jmlr.org/papers/v18/16-537.html}
}


@inproceedings{Quinlan1992,
  title={Learning with continuous classes},
  author={Quinlan, John R and others},
  booktitle={5th Australian joint conference on artificial intelligence},
  volume={92},
  pages={343--348},
  year={1992},
  organization={Singapore}
}

@book{Rasmussen2006,
 author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
 title = {Gaussian Processes for Machine Learning},
 year = {2006},
 isbn = {026218253X},
 publisher = {The MIT Press},
}

@ARTICLE{Susto2015, 
  author={G. A. Susto and A. Schirru and S. Pampuri and S. McLoone and A. Beghi}, 
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Machine Learning for Predictive Maintenance: A Multiple Classifier Approach}, 
  year={2015}, 
  volume={11}, 
  number={3}, 
  pages={812-820}, 
  abstract={In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.}, 
  keywords={data mining;learning (artificial intelligence);pattern classification;production engineering computing;semiconductor device manufacture;PdM;censored data problem;data mining;dynamical decision rules;health factors;high-dimensional problem;maintenance management;multiple classifier machine learning methodology;operating cost-based maintenance decision system;predictive maintenance;quantitative indicators;semiconductor manufacturing maintenance problem;Availability;Informatics;Manufacturing;Predictive maintenance;Production;Training;Classification algorithms;data mining;ion implantation;machine learning (ML);predictive maintenance (PdM);semiconductor device manufacture}, 
  doi={10.1109/TII.2014.2349359}, 
  ISSN={1551-3203}, 
  month={June},
}

@inbook{Tiger2018-gp-motion-pattern, 
  title={Gaussian Process Based Motion Pattern Recognition with Sequential Local Models}, 
  url={http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-148724}, 
  abstractNote={Conventional trajectory-based vehicular traffic analysis approaches work 
    well in simple environments such as a single crossing but they do not scale to more 
    structurally complex environments such as networks of interconnected crossings 
    (e.g. urban road networks). Local trajectory models are necessary to cope with 
    the multi-modality of such structures, which in turn introduces new challenges. 
    These larger and more complex environments increase the occurrences of non-consistent 
    lack of motion and self-overlaps in observed trajectories which impose further challenges. 
    In this paper we consider the problem of motion pattern recognition in the setting of 
    sequential local motion pattern models. That is, classifying sub-trajectories from observed 
    trajectories in accordance with which motion pattern that best explains it. 
    We introduce a Gaussian process (GP) based modeling approach which outperforms the 
    state-of-the-art GP based motion pattern approaches at this task.
    We investigate the impact of varying local model overlap and the length of the observed 
    trajectory trace on the classification quality. We further show that introducing a pre-processing 
    step filtering out stops from the training data significantly improves the classification 
    performance. The approach is evaluated using real GPS position data from city buses driving 
    in urban areas for extended periods of time.}, 
  booktitle={2018 IEEE Intelligent Vehicles Symposium (IV)}, 
  author={Tiger, Mattias and Heintz, Fredrik}, 
  year={2018}
}

@article{Willmott2005,
  title={Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},
  author={Willmott, Cort J and Matsuura, Kenji},
  journal={Climate research},
  volume={30},
  number={1},
  pages={79--82},
  year={2005}
}

@Article{Chai2014,
AUTHOR = {Chai, T. and Draxler, R. R.},
TITLE = {Root mean square error (RMSE) or mean absolute error (MAE)? Arguments against avoiding RMSE in the literature},
JOURNAL = {Geoscientific Model Development},
VOLUME = {7},
YEAR = {2014},
NUMBER = {3},
PAGES = {1247--1250},
URL = {https://www.geosci-model-dev.net/7/1247/2014/},
DOI = {10.5194/gmd-7-1247-2014}
}


@book{Tukey1977,
  title={Exploratory data analysis},
  author={Tukey, John W},
  year={1977},
  publisher={Reading, Mass.}
}

@book{Velleman1981,
  title={Applications, basics, and computing of exploratory data analysis},
  author={Velleman, Paul F and Hoaglin, David C},
  year={1981},
  publisher={Duxbury Press},
  isbn={0-87150-409-X},
  url={http://hdl.handle.net/1813/78},
  abstract={This book provides an introduction to the methods
  of exploratory data analysis as originally developed by John Tukey. For each of nine methods it discusses and illustrates the methods, provides background derivations and explanations, and presents programs in Fortran and BASIC. For most of these methods, this was the first source of such programs.},
}

@article{Wold1987,
  title = "Principal component analysis",
  journal = "Chemometrics and Intelligent Laboratory Systems",
  volume = "2",
  number = "1",
  pages = "37 - 52",
  year = "1987",
  note = "Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists",
  issn = "0169-7439",
  doi = "https://doi.org/10.1016/0169-7439(87)80084-9",
  url = "http://www.sciencedirect.com/science/article/pii/0169743987800849",
  author = "Svante Wold and Kim Esbensen and Paul Geladi"
}

###############################################################################
###############################################################################
#              EXAMPLES OF APPLICATIONS USING GAUSSIAN PROCESSES              #
###############################################################################
###############################################################################

@ARTICLE{Alaa2018, 
author={A. M. Alaa and J. Yoon and S. Hu and M. van der Schaar}, 
journal={IEEE Transactions on Biomedical Engineering}, 
title={Personalized Risk Scoring for Critical Care Prognosis Using Mixtures of Gaussian Processes}, 
year={2018}, 
volume={65}, 
number={1}, 
pages={207-218}, 
abstract={Objective: In this paper, we develop a personalized real-time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their (temporal) lab tests and vital signs; the proposed risk scoring system ensures timely intensive care unit admissions for clinically deteriorating patients. Methods: The risk scoring system is based on the idea of sequential hypothesis testing under an uncertain time horizon. The system learns a set of latent patient subtypes from the offline electronic health record data, and trains a mixture of Gaussian Process experts, where each expert models the physiological data streams associated with a specific patient subtype. Transfer learning techniques are used to learn the relationship between a patient's latent subtype and her static admission information (e.g., age, gender, transfer status, ICD-9 codes, etc). Results: Experiments conducted on data from a heterogeneous cohort of 6321 patients admitted to Ronald Reagan UCLA medical center show that our score significantly outperforms the currently deployed risk scores, such as the Rothman index, MEWS, APACHE, and SOFA scores, in terms of timeliness, true positive rate, and positive predictive value. Conclusion: Our results reflect the importance of adopting the concepts of personalized medicine in critical care settings; significant accuracy and timeliness gains can be achieved by accounting for the patients' heterogeneity. Significance: The proposed risk scoring methodology can confer huge clinical and social benefits on a massive number of critically ill inpatients who exhibit adverse outcomes including, but not limited to, cardiac arrests, respiratory arrests, and septic shocks.}, 
keywords={diseases;Gaussian processes;health care;learning (artificial intelligence);medical computing;medical information systems;patient care;patient diagnosis;patient monitoring;risk analysis;critical care prognosis;Gaussian processes;real-time risk scoring algorithm;timely assessments;granular assessments;clinical acuity;ward patients;lab tests;risk scoring system;timely intensive care unit;sequential hypothesis testing;uncertain time horizon;latent patient subtypes;offline electronic health record data;Gaussian Process experts;expert models the physiological data streams;specific patient subtype;transfer learning techniques;static admission information;currently deployed risk scores;SOFA scores;personalized medicine;critical care settings;risk scoring methodology;personalized risk scoring;Physiology;Biomedical monitoring;Indexes;Computational modeling;Real-time systems;Testing;Data models;Critical care medicine;personalized medicine;prognosis;sequential hypothesis testing}, 
doi={10.1109/TBME.2017.2698602}, 
ISSN={0018-9294}, 
month={01},}

@article{Angus2017,
  title={Inferring probabilistic stellar rotation periods using Gaussian processes},
  author={Angus, Ruth and Morton, Timothy and Aigrain, Suzanne and Foreman-Mackey, Daniel and Rajpaul, Vinesh},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={474},
  number={2},
  pages={2094--2108},
  year={2017},
  publisher={Oxford University Press}
}

@article{Czekala2017,
  author={Ian Czekala and Kaisey S. Mandel and Sean M. Andrews and Jason A. Dittmann and Sujit K. Ghosh and Benjamin T.
Montet and Elisabeth R. Newton},
  title={Disentangling Time-series Spectra with Gaussian Processes: Applications to Radial Velocity Analysis},
  journal={The Astrophysical Journal},
  volume={840},
  number={1},
  pages={49},
  url={http://stacks.iop.org/0004-637X/840/i=1/a=49},
  year={2017},
  abstract={Measurements of radial velocity variations from the spectroscopic monitoring of stars and their companions are essential for a broad swath of astrophysics; these measurements provide access to the fundamental physical properties that dictate all phases of stellar evolution and facilitate the quantitative study of planetary systems. The conversion of those measurements into both constraints on the orbital architecture and individual component spectra can be a serious challenge, however, especially for extreme flux ratio systems and observations with relatively low sensitivity. Gaussian processes define sampling distributions of flexible, continuous functions that are well-motivated for modeling stellar spectra, enabling proficient searches for companion lines in time-series spectra. We introduce a new technique for spectral disentangling, where the posterior distributions of the orbital parameters and intrinsic, rest-frame stellar spectra are explored simultaneously without needing to invoke cross-correlation templates. To demonstrate its potential, this technique is deployed on red-optical time-series spectra of the mid-M-dwarf binary LP661-13. We report orbital parameters with improved precision compared to traditional radial velocity analysis and successfully reconstruct the primary and secondary spectra. We discuss potential applications for other stellar and exoplanet radial velocity techniques and extensions to time-variable spectra. The code used in this analysis is freely available as an open-source Python package.}
}

@article{Dong2017,
  author    = {Jing Dong and Byron Boots and Frank Dellaert},
  title     = {Sparse Gaussian Processes for Continuous-Time Trajectory Estimation on Matrix Lie Groups},
  journal   = {CoRR},
  volume    = {abs/1705.06020},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06020},
}

@article{Finley2017,
author = {O. Finley, Andrew and Datta, Abhirup and C. Cook, Bruce and Morton, Douglas and E. Andersen, Hans and Banerjee, Sudipto},
year = {2017},
month = {02},
pages = {},
url = {http://arxiv.org/abs/1702.00434},
title = {Applying Nearest Neighbor Gaussian Processes to Massive Spatial Data Sets: Forest Canopy Height Prediction Across Tanana Valley Alaska}
}

@article{Raissi2017,
title = "Machine learning of linear differential equations using Gaussian processes",
journal = "Journal of Computational Physics",
volume = "348",
pages = "683 - 693",
year = "2017",
issn = "0021-9991",
doi = "https://doi.org/10.1016/j.jcp.2017.07.050",
url = "http://www.sciencedirect.com/science/article/pii/S0021999117305582",
author = "Maziar Raissi and Paris Perdikaris and George Em Karniadakis",
keywords = "Probabilistic machine learning, Inverse problems, Fractional differential equations, Uncertainty quantification, Functional genomics",
abstract = "This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or âblack-boxâ computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics."
}

@article{Stein2017,
title = {Spatio-Temporal Data Analysis at Scale Using Models Based on Gaussian Processes},
author = {Stein, Michael},
abstractNote = {Gaussian processes are the most commonly used statistical model for spatial and spatio-temporal processes that vary continuously. They are broadly applicable in the physical sciences and engineering and are also frequently used to approximate the output of complex computer models, deterministic or stochastic. We undertook research related to theory, computation, and applications of Gaussian processes as well as some work on estimating extremes of distributions for which a Gaussian process assumption might be inappropriate. Our theoretical contributions include the development of new classes of spatial-temporal covariance functions with desirable properties and new results showing that certain covariance models lead to predictions with undesirable properties. To understand how Gaussian process models behave when applied to deterministic computer models, we derived what we believe to be the first significant results on the large sample properties of estimators of parameters of Gaussian processes when the actual process is a simple deterministic function. Finally, we investigated some theoretical issues related to maxima of observations with varying upper bounds and found that, depending on the circumstances, standard large sample results for maxima may or may not hold. Our computational innovations include methods for analyzing large spatial datasets when observations fall on a partially observed grid and methods for estimating parameters of a Gaussian process model from observations taken by a polar-orbiting satellite. In our application of Gaussian process models to deterministic computer experiments, we carried out some matrix computations that would have been infeasible using even extended precision arithmetic by focusing on special cases in which all elements of the matrices under study are rational and using exact arithmetic. The applications we studied include total column ozone as measured from a polar-orbiting satellite, sea surface temperatures over the Pacific Ocean, and annual temperature extremes at a site in New York City. In each of these applications, our theoretical and computational innovations were directly motivated by the challenges posed by analyzing these and similar types of data.},
doi = {10.2172/1346562},
journal = {},
place = {United States},
year = {2017},
month = {3}
}

@incollection{NIPS2017_6877,
title = {Convolutional Gaussian Processes},
author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2849--2858},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6877-convolutional-gaussian-processes.pdf}
}


@ARTICLE{8082124, 
author={P. Morales-Álvarez and A. Pérez-Suay and R. Molina and G. Camps-Valls}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Remote Sensing Image Classification With Large-Scale Gaussian Processes}, 
year={2018}, 
volume={56}, 
number={2}, 
pages={1103-1114}, 
abstract={Current remote sensing image classification problems have to deal with an unprecedented amount of heterogeneous and complex data sources. Upcoming missions will soon provide large data streams that will make land cover/use classification difficult. Machine-learning classifiers can help at this, and many methods are currently available. A popular kernel classifier is the Gaussian process classifier (GPC), since it approaches the classification problem with a solid probabilistic treatment, thus yielding confidence intervals for the predictions as well as very competitive results to the state-of-the-art neural networks and support vector machines. However, its computational cost is prohibitive for large-scale applications, and constitutes the main obstacle precluding wide adoption. This paper tackles this problem by introducing two novel efficient methodologies for GP classification. We first include the standard random Fourier features approximation into GPC, which largely decreases its computational cost and permits large-scale remote sensing image classification. In addition, we propose a model which avoids randomly sampling a number of Fourier frequencies and alternatively learns the optimal ones within a variational Bayes approach. The performance of the proposed methods is illustrated in complex problems of cloud detection from multispectral imagery and infrared sounding data. Excellent empirical results support the proposal in both computational cost and accuracy.}, 
keywords={Bayes methods;Gaussian processes;geophysical image processing;image classification;learning (artificial intelligence);remote sensing;support vector machines;GPC;computational cost;large-scale Gaussian processes;data streams;Gaussian process classifier;neural networks;support vector machines;GP classification;land cover classification;remote sensing image classification problems;kernel classifier;Fourier frequencies;variational Bayes approach;standard random Fourier feature approximation;land use classification;Kernel;Remote sensing;Computational efficiency;Standards;Image resolution;Support vector machines;Gaussian processes;Cloud detection;Gaussian process classification (GPC);IAVISA;Infrared Atmospheric Sounding Interferometer (IASI)/Advanced Very High Resolution Radiometer (AVHRR);random Fourier features (RFFs);Spinning Enhanced Visible and Infrared Imager/Meteosat Second Generation (SEVIRI/MSG);variational inference}, 
doi={10.1109/TGRS.2017.2758922}, 
ISSN={0196-2892}, 
month={02},}